# Implementation Plan: Movie Recommendation System

## Overview

This document breaks down each phase into granular steps. Use this to track progress and resume work in new conversation threads.

**Status Legend:**
- â¬œ Not started
- ðŸŸ¡ In progress
- âœ… Completed

---

## Phase 1: Project Skeleton

**Goal:** Create foundation with proper structure, dependencies, tooling, and containerization.

| Step | Task | Files Created | Status |
|------|------|---------------|--------|
| 1.1 | Create `pyproject.toml` with tool configs | `pyproject.toml` | âœ… |
| 1.2 | Create requirements files | `requirements.txt`, `requirements-dev.txt` | âœ… |
| 1.3 | Create directory structure and `__init__.py` files | `src/*/`, `data/*/`, `models/*/`, `tests/`, `configs/`, `scripts/`, `.github/workflows/` | âœ… |
| 1.4 | Create `.gitignore` | `.gitignore` | âœ… |
| 1.5 | Create config YAML files | `configs/data.yaml`, `configs/training.yaml`, `configs/serving.yaml`, `configs/monitoring.yaml`, `configs/retrain.yaml` | âœ… |
| 1.6 | Create Makefile | `Makefile` | âœ… |
| 1.7 | Create pre-commit config | `.pre-commit-config.yaml` | âœ… |
| 1.8 | Create Dockerfile | `Dockerfile` | âœ… |
| 1.9 | Create docker-compose | `docker-compose.yml` | âœ… |
| 1.10 | Verification and README | `README.md`, verify all commands work | âœ… |

---

## Phase 2: Data Pipeline

**Goal:** Download MovieLens data, preprocess into implicit feedback, split by time, generate features.

| Step | Task | Files Created | Status |
|------|------|---------------|--------|
| 2.1 | Create config loader utility | `src/config.py` | âœ… |
| 2.2 | Implement data download | `src/data/download.py` | âœ… |
| 2.3 | Implement preprocessing (implicit conversion, filtering) | `src/data/preprocess.py` | âœ… |
| 2.4 | Implement time-based splitting | `src/data/split.py` | âœ… |
| 2.5 | Implement feature building (genre encoding, popularity) | `src/features/build.py` | âœ… |
| 2.6 | Create ID mapping utilities | `src/data/mappings.py` | âœ… |
| 2.7 | Write unit tests for data pipeline | `tests/test_data.py` | âœ… |
| 2.8 | Verification: end-to-end data pipeline | Run `make download-data process-data build-features` | âœ… |

**Outputs:**
- `data/raw/` â€” Original MovieLens CSVs
- `data/processed/` â€” `interactions.csv`, `train.csv`, `val.csv`, `test.csv`
- `data/features/` â€” `item_features.csv`, `popularity.csv`
- `data/processed/` â€” `user_mapping.json`, `item_mapping.json`

---

## Phase 3: Model Training

**Goal:** Implement recommender models, evaluation metrics, MLflow tracking, model registry.

| Step | Task | Files Created | Status |
|------|------|---------------|--------|
| 3.1 | Create abstract base recommender class | `src/models/base.py` | âœ… |
| 3.2 | Implement item-item similarity model | `src/models/item_item.py` | âœ… |
| 3.3 | Implement ALS matrix factorization model | `src/models/als.py` | âœ… |
| 3.4 | Implement FAISS index builder | `src/models/index.py` | âœ… |
| 3.5 | Implement ranking metrics (precision, recall, NDCG, MRR) | `src/training/evaluate.py` | âœ… |
| 3.6 | Implement MLflow utilities (logging, registry) | `src/training/mlflow_utils.py` | âœ… |
| 3.7 | Implement training orchestrator | `src/training/train.py` | âœ… |
| 3.8 | Implement model export (save production artifacts) | `src/training/export.py` | âœ… |
| 3.9 | Write unit tests for models and evaluation | `tests/test_models.py`, `tests/test_evaluation.py` | âœ… |
| 3.10 | Verification: train and register model | Run `make train`, verify MLflow UI | âœ… |

**Outputs:**
- MLflow experiment with logged runs
- `models/production/` â€” `model_metadata.json`, `user_embeddings.npy`, `item_embeddings.npy`, `item_index.faiss`, `user_mapping.json`, `item_mapping.json`, `item_features.json`

---

## Phase 4: API Serving

**Goal:** Create FastAPI service with recommendation endpoints, request logging.

| Step | Task | Files Created | Status |
|------|------|---------------|--------|
| 4.1 | Define Pydantic schemas (request/response models) | `src/api/schemas.py` | âœ… |
| 4.2 | Implement model loader (load artifacts at startup) | `src/api/model_loader.py` | âœ… |
| 4.3 | Implement recommendation engine (scoring, filtering) | `src/api/recommender.py` | âœ… |
| 4.4 | Implement fallback handler (cold-start logic) | `src/api/fallback.py` | âœ… |
| 4.5 | Implement request logger (SQLite async writes) | `src/api/logger.py` | âœ… |
| 4.6 | Create FastAPI app with all endpoints | `src/api/main.py` | âœ… |
| 4.7 | Write API tests | `tests/test_api.py` | âœ… |
| 4.8 | Verification: test all endpoints | Run `make serve`, test with curl/httpx | âœ… |

**Endpoints:**
- `GET /health` â€” Health check
- `POST /recommend` â€” Personalized recommendations
- `POST /similar` â€” Item similarity
- `GET /popular` â€” Popular items fallback

---

## Phase 5: User-Facing Frontend

**Goal:** Create a Streamlit-based web interface for users to interact with the recommendation system.

| Step | Task | Files Created | Status |
|------|------|---------------|--------|
| 5.1 | Create frontend directory structure | `src/frontend/__init__.py` | â¬œ |
| 5.2 | Implement API client utilities | `src/frontend/api_client.py` | â¬œ |
| 5.3 | Implement personalized recommendations page | `src/frontend/pages/personalized.py` | â¬œ |
| 5.4 | Implement similar movies page | `src/frontend/pages/similar.py` | â¬œ |
| 5.5 | Implement popular movies page | `src/frontend/pages/popular.py` | â¬œ |
| 5.6 | Implement about/status page | `src/frontend/pages/about.py` | â¬œ |
| 5.7 | Create main Streamlit app with navigation | `src/frontend/app.py` | â¬œ |
| 5.8 | Add styling and UI polish | CSS customizations, loading states | â¬œ |
| 5.9 | Write frontend tests | `tests/test_frontend.py` | â¬œ |
| 5.10 | Verification: end-to-end user flow | Test all pages manually | â¬œ |

**Pages:**
- **Personalized** â€” Enter user ID, get recommendations
- **Find Similar** â€” Select a movie, find similar ones
- **Popular** â€” Browse popular movies with genre filter
- **About** â€” System status, how it works

---

## Phase 6: Monitoring & Retraining

**Goal:** Implement KPI computation, replay evaluation, dashboard, retraining pipeline.

| Step | Task | Files Created | Status |
|------|------|---------------|--------|
| 6.1 | Implement KPI computation (latency, traffic, coverage) | `src/monitoring/kpis.py` | âœ… |
| 6.2 | Implement replay evaluation (offline metrics on logs) | `src/monitoring/replay_eval.py` | âœ… |
| 6.3 | Implement Streamlit monitoring dashboard | `src/monitoring/dashboard.py` | â¬œ |
| 6.4 | Implement data merge for retraining | `src/pipeline/data_merge.py` | â¬œ |
| 6.5 | Implement promotion logic | `src/pipeline/promote.py` | â¬œ |
| 6.6 | Implement retraining orchestrator | `src/pipeline/retrain.py` | â¬œ |
| 6.7 | Write tests for monitoring and pipeline | `tests/test_monitoring.py`, `tests/test_pipeline.py` | â¬œ |
| 6.8 | Verification: full retraining cycle | Run `make retrain`, verify promotion | â¬œ |

**Outputs:**
- `data/logs/reports/` â€” KPI reports
- Streamlit dashboard on port 8501
- Automated model promotion workflow

---

## Phase 7: CI/CD

**Goal:** Implement pre-commit hooks, GitHub Actions, finalize tests.

| Step | Task | Files Created | Status |
|------|------|---------------|--------|
| 7.1 | Configure pre-commit hooks | `.pre-commit-config.yaml` (if not done in 1.7) | â¬œ |
| 7.2 | Create GitHub Actions CI workflow | `.github/workflows/ci.yml` | â¬œ |
| 7.3 | Ensure all tests pass | Run `make check` | â¬œ |
| 7.4 | Create GitHub Actions Docker build workflow | `.github/workflows/docker.yml` | â¬œ |
| 7.5 | Final documentation | Update `README.md` with usage instructions | â¬œ |
| 7.6 | Verification: push to GitHub, verify Actions pass | Push code, check Actions tab | â¬œ |

**Outputs:**
- Pre-commit hooks running on every commit
- GitHub Actions running on every push/PR
- Complete, documented project

---

## Quick Reference: File to Phase Mapping

| Directory | Files | Phase |
|-----------|-------|-------|
| `src/data/` | `download.py`, `preprocess.py`, `split.py`, `mappings.py` | 2 |
| `src/features/` | `build.py` | 2 |
| `src/models/` | `base.py`, `item_item.py`, `als.py`, `index.py` | 3 |
| `src/training/` | `train.py`, `evaluate.py`, `mlflow_utils.py`, `export.py` | 3 |
| `src/api/` | `main.py`, `schemas.py`, `model_loader.py`, `recommender.py`, `fallback.py`, `logger.py` | 4 |
| `src/frontend/` | `app.py`, `api_client.py`, `pages/*.py` | 5 |
| `src/monitoring/` | `kpis.py`, `replay_eval.py`, `dashboard.py` | 6 |
| `src/pipeline/` | `retrain.py`, `promote.py`, `data_merge.py` | 6 |
| `.github/workflows/` | `ci.yml`, `docker.yml` | 7 |

---

## Running the Complete System

After all phases are complete, run the system with:

```bash
# Terminal 1: Start the API
python -m src.api.main
# API available at http://localhost:8000

# Terminal 2: Start the User Frontend
streamlit run src/frontend/app.py --server.port 8502
# Frontend available at http://localhost:8502

# Terminal 3: Start the Monitoring Dashboard
streamlit run src/monitoring/dashboard.py --server.port 8501
# Dashboard available at http://localhost:8501
```

Or use Docker Compose:

```bash
docker-compose up
```
